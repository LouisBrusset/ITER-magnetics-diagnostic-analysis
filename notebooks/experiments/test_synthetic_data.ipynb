{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca06f38",
   "metadata": {},
   "source": [
    "## MSCRED (Multi Scale Convolutional Recurrent Encoder Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70a265",
   "metadata": {},
   "source": [
    "Ce script vise  a tester toutes les briques qui compose le pipeline MSCRED sur des données fictives pour ensuite créer le module qui sera utile dans notre projet de détection d'anomalie dans les flux de diagnostiques magnétique de tokamak !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd40cf",
   "metadata": {},
   "source": [
    "### 1. Création des jeux de données et visualisation\n",
    "\n",
    "1. Créer des signaux multivariés aléatoires et les visualiser. (ils constitueront le jeu d'entrainement)\n",
    "2. Reprendre ce jeu synthétique et pour un sous-groupe, y ajouter des valeurs abérantes pour qu'ils soient les anomalies à détecter. Labeliser le tout (constituera le jeu de test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11dc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multivariate_ts_1(n_variables, n_timesteps, noise_level=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    Generates realistic multivariate time series with temporal continuity and noise.\n",
    "    \n",
    "    Args:\n",
    "        n_variables (int): Number of time series (variables)\n",
    "        n_timesteps (int): Length of time series\n",
    "        noise_level (float): Gaussian noise level (standard deviation)\n",
    "        seed (int): Seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Matrix of time series of shape (n_variables, n_timesteps)\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Matrice initialization\n",
    "    data = np.zeros((n_variables, n_timesteps))\n",
    "    \n",
    "    # Unique parameters for each variable\n",
    "    means = np.random.uniform(-10, 10, n_variables)             # Means\n",
    "    volatilities = np.random.uniform(0.1, 1.0, n_variables)     # Volatilities\n",
    "    trends = np.random.uniform(-0.05, 0.05, n_variables)        # Trends\n",
    "    frequencies = np.random.uniform(0.01, 0.2, n_variables)     # frequencies\n",
    "    \n",
    "    for i in range(n_variables):\n",
    "        # Random generation of the base time series (AR(1) process)\n",
    "        base = np.zeros(n_timesteps)\n",
    "        base[0] = np.random.normal(means[i], volatilities[i])\n",
    "        \n",
    "        # Adding temporal continuity\n",
    "        for t in range(1, n_timesteps):\n",
    "            # Autoregressive component with trend and seasonality\n",
    "            drift = trends[i] * t\n",
    "            season = 0.5 * np.sin(2 * np.pi * frequencies[i] * t)\n",
    "            innovation = np.random.normal(0, volatilities[i])\n",
    "            \n",
    "            base[t] = (0.8 * base[t-1] + 0.2 * base[0] + \n",
    "                       drift + season + innovation)\n",
    "        \n",
    "        data[i] = base\n",
    "    \n",
    "    # Adding Gaussian noise\n",
    "    noise = np.random.normal(0, noise_level, (n_variables, n_timesteps))\n",
    "    return data + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824726f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multivariate_ts_2(n_variables: int, n_timesteps: int, noise_level: float = 0.1, seed:bool = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Improved version with amplitude control and large-scale sinusoids.\n",
    "    \n",
    "    Args:\n",
    "        n_variables (int): Number of time series\n",
    "        n_timesteps (int): Length of series\n",
    "        noise_level (float): Noise level (standard deviation)\n",
    "        seed (int): Seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Matrix of shape (n_variables, n_timesteps)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    data = np.zeros((n_variables, n_timesteps), dtype=np.float32)\n",
    "    time = np.arange(n_timesteps, dtype=np.float32)\n",
    "    \n",
    "    #Unique parameters for each variable\n",
    "    base_amps = np.random.uniform(5, 20, n_variables)  # Amplitude de base\n",
    "    slow_freqs = np.random.uniform(0.001, 0.01, n_variables)  # Fréquences lentes\n",
    "    fast_freqs = np.random.uniform(0.05, 0.2, n_variables)  # Fréquences rapides\n",
    "    phases = np.random.uniform(0, 2*np.pi, n_variables)  # Déphasages\n",
    "    \n",
    "    for i in range(n_variables):\n",
    "        # Primary component (slow sinusoid)\n",
    "        slow_wave = base_amps[i] * np.sin(2 * np.pi * slow_freqs[i] * time + phases[i])\n",
    "        \n",
    "        # Secondary component (fast sinusoid)\n",
    "        fast_wave = 0.5 * base_amps[i] * np.sin(2 * np.pi * fast_freqs[i] * time)\n",
    "        \n",
    "        # Autoregressive noise component\n",
    "        # AR(1) process with a random walk component\n",
    "        ar_noise = np.zeros(n_timesteps)\n",
    "        ar_noise[0] = np.random.normal(0, 1)\n",
    "        for t in range(1, n_timesteps):\n",
    "            ar_noise[t] = 0.7 * ar_noise[t-1] + np.random.normal(0, 0.5)\n",
    "        \n",
    "        # Combining components\n",
    "        data[i] = slow_wave + fast_wave + ar_noise\n",
    "    \n",
    "    # Mesurement noise\n",
    "    noise = np.random.normal(0, noise_level, (n_variables, n_timesteps))\n",
    "    data = data + noise\n",
    "    return (data - np.mean(data, axis=1, keepdims=True)) / np.std(data, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4512c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multivariate_ts_3(\n",
    "    n_variables: int,\n",
    "    n_timesteps: int,\n",
    "    noise_level: float = 0.3,\n",
    "    min_delay: int = 50,\n",
    "    max_delay: int = 100,\n",
    "    min_freq: int = 40,\n",
    "    max_freq: int = 50,\n",
    "    seed: int = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates synthetic multivariate time series with trigonometric patterns and noise.\n",
    "    Implementation like in the MSCRED paper.\n",
    "\n",
    "    Args:\n",
    "        n_variables: Number of time series (variables/sensors)\n",
    "        n_timesteps: Length of time series\n",
    "        noise_level: Gaussian noise level\n",
    "        min_delay: Minimum time delay (t0)\n",
    "        max_delay: Maximum time delay (t0)\n",
    "        min_freq: Minimum angular frequency (ω)\n",
    "        max_freq: Maximum angular frequency (ω)\n",
    "        seed: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (n_variables, n_timesteps)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Generate random parameters for each variable\n",
    "    s_rand = np.random.randint(0, 2, size=n_variables)  # 0=sin, 1=cos\n",
    "    t0 = np.random.randint(min_delay, max_delay + 1, size=n_variables)\n",
    "    omega = np.random.randint(min_freq, max_freq + 1, size=n_variables)\n",
    "    \n",
    "    # Initialize the time series data matrix\n",
    "    ts_data = np.zeros((n_variables, n_timesteps))\n",
    "    time_index = np.arange(n_timesteps)\n",
    "    \n",
    "    for i in range(n_variables):\n",
    "        base = (time_index - t0[i]) / omega[i]\n",
    "        if s_rand[i] == 0:\n",
    "            trig_component = np.sin(base)\n",
    "        else:\n",
    "            trig_component = np.cos(base)\n",
    "        \n",
    "        # Add gaussian noise\n",
    "        noise = noise_level * np.random.randn(n_timesteps)\n",
    "        ts_data[i] = trig_component + noise\n",
    "    \n",
    "    return ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = generate_multivariate_ts_1(\n",
    "    n_variables=10,\n",
    "    n_timesteps=200,\n",
    "    noise_level=0.3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(data_train.shape[0]):\n",
    "    plt.plot(data_train[i], label=f'{i+1}th serie')\n",
    "plt.title('Generated Multivariate Time Series')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = generate_multivariate_ts_3(\n",
    "    n_variables=10,\n",
    "    n_timesteps=200,\n",
    "    noise_level=0.3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(data_train.shape[0]):\n",
    "    plt.plot(data_train[i], label=f'{i+1}th serie')\n",
    "plt.title('Generated Multivariate Time Series')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4dfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = generate_multivariate_ts_2(\n",
    "    n_variables=10,\n",
    "    n_timesteps=200,\n",
    "    noise_level=0.7,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(data_train.shape[0]):\n",
    "    plt.plot(data_train[i], label=f'{i+1}th serie')\n",
    "plt.title('Generated Multivariate Time Series')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = generate_multivariate_ts_2(\n",
    "    n_variables=32,\n",
    "    n_timesteps=20000,\n",
    "    noise_level=0.7,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(data_train.shape[0]):\n",
    "    plt.plot(data_train[i], label=f'{i+1}th serie')\n",
    "plt.title('Generated Multivariate Time Series')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomalies(data, start_index=15000, duration_range=(50, 200), \n",
    "                    n_anomalies=5, anomaly_strength=1.5, seed=None):\n",
    "    \"\"\"\n",
    "    Crée plusieurs anomalies dans les séries temporelles multivariées.\n",
    "    \n",
    "    Args:\n",
    "        data: Matrice de shape (n_variables, n_timesteps)\n",
    "        start_index: Index de début des anomalies\n",
    "        duration_range: Tuple (min_duration, max_duration) pour la longueur des anomalies\n",
    "        n_anomalies: Nombre d'anomalies à créer\n",
    "        anomaly_strength: Force de l'anomalie (multiplicateur de l'écart-type)\n",
    "        seed: Seed pour la reproductibilité\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Données avec anomalies\n",
    "        list: Liste des informations sur les anomalies créées\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    data_with_anomalies = data.copy()\n",
    "    n_variables, n_timesteps = data.shape\n",
    "    anomalies_info = []\n",
    "    \n",
    "    for i in range(n_anomalies):\n",
    "        start_anomaly = np.random.randint(start_index, n_timesteps)\n",
    "        series_num = np.random.randint(0, n_variables)\n",
    "        duration = np.random.randint(duration_range[0], duration_range[1])\n",
    "\n",
    "        end_anomaly = min(start_anomaly + duration, n_timesteps)\n",
    "        actual_duration = end_anomaly - start_anomaly\n",
    "\n",
    "        if actual_duration <= 0:\n",
    "            i -= 1\n",
    "            continue\n",
    "\n",
    "        local_std = np.std(data[series_num, start_anomaly-100:start_anomaly])\n",
    "        anomaly_type = np.random.choice(['spike', 'drift', 'level_shift', 'seasonal_break'])\n",
    "        \n",
    "        if anomaly_type == 'spike':\n",
    "            spike_value = anomaly_strength * local_std * np.random.choice([-1, 1])\n",
    "            data_with_anomalies[series_num, start_anomaly:end_anomaly] += spike_value\n",
    "\n",
    "        elif anomaly_type == 'drift':\n",
    "            drift_slope = anomaly_strength * local_std * 0.01 * np.random.choice([-1, 1])\n",
    "            drift_values = np.arange(actual_duration) * drift_slope\n",
    "            data_with_anomalies[series_num, start_anomaly:end_anomaly] += drift_values\n",
    "\n",
    "        elif anomaly_type == 'level_shift':\n",
    "            shift_value = anomaly_strength * local_std * np.random.choice([-1, 1])\n",
    "            data_with_anomalies[series_num, start_anomaly:end_anomaly] += shift_value\n",
    "\n",
    "        elif anomaly_type == 'seasonal_break':\n",
    "            seasonal_amp = anomaly_strength * local_std * 0.5\n",
    "            freq_change = np.random.uniform(0.5, 2.0)\n",
    "            time_segment = np.arange(actual_duration)\n",
    "            seasonal_anomaly = seasonal_amp * np.sin(2 * np.pi * freq_change * time_segment / 100)\n",
    "            data_with_anomalies[series_num, start_anomaly:end_anomaly] += seasonal_anomaly\n",
    "\n",
    "        anomalies_info.append({\n",
    "            'series': series_num,\n",
    "            'start_index': start_anomaly,\n",
    "            'duration': actual_duration,\n",
    "            'type': anomaly_type,\n",
    "            'strength': anomaly_strength\n",
    "        })\n",
    "    \n",
    "    return data_with_anomalies, anomalies_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb979ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_anomalous, anomalies_info = create_anomalies(\n",
    "    data=data_train,\n",
    "    start_index=15000,\n",
    "    duration_range=(80, 150),\n",
    "    n_anomalies=8,\n",
    "    anomaly_strength=2.0,\n",
    "    seed=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f888e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(data_normal, data_anomalous, anomalies_info, n_series_to_plot=3):\n",
    "    \"\"\"\n",
    "    Visualise les séries avec anomalies.\n",
    "    \"\"\"\n",
    "    n_variables, n_timesteps = data_normal.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(n_series_to_plot, 1, figsize=(12, 3*n_series_to_plot))\n",
    "    if n_series_to_plot == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    series_to_plot = np.random.choice(n_variables, n_series_to_plot, replace=False)\n",
    "    \n",
    "    for i, series_num in enumerate(series_to_plot):\n",
    "        axes[i].plot(data_normal[series_num], 'b-', alpha=0.7, label='Normal')\n",
    "        axes[i].plot(data_anomalous[series_num], 'r-', alpha=0.9, label='With Anomalies')\n",
    "        \n",
    "        for anomaly in anomalies_info:\n",
    "            if anomaly['series'] == series_num:\n",
    "                start = anomaly['start_index']\n",
    "                end = start + anomaly['duration']\n",
    "                axes[i].axvspan(start, end, color='yellow', alpha=0.3, label='Anomaly' if i == 0 else \"\")\n",
    "        \n",
    "        axes[i].set_title(f'Série {series_num}')\n",
    "        axes[i].set_xlabel('Time')\n",
    "        axes[i].set_ylabel('Value')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_anomalies(data_train, data_val_anomalous, anomalies_info, n_series_to_plot=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2afb3b7",
   "metadata": {},
   "source": [
    "### 2. Generate \"correlation\" matrixes from thosee times series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f44bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_signature_matrix(\n",
    "    data: np.ndarray,\n",
    "    win_size: list[int] = [10, 30, 60],\n",
    "    min_time: int = 0,\n",
    "    max_time: int = None,\n",
    "    gap_time: int = 10,\n",
    "    normalize: bool = True,\n",
    "    save_path: str = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates multi-scale signature matrices from multivariate time series.\n",
    "    \n",
    "    Args:\n",
    "        data: Input data in the form (n_sensors, n_timesteps)\n",
    "        win_size: List of window sizes\n",
    "        min_time: Starting time point\n",
    "        max_time: Ending time point (if None, uses total length)\n",
    "        gap_time: Interval between segments\n",
    "        normalize: If True, applies min-max normalization\n",
    "        save_path: Path to save matrices (if None, no save)\n",
    "    \n",
    "    Returns:\n",
    "        4D array of signature matrices with shape [time, len(win_sizes), n_sensors, n_sensors]\n",
    "    \"\"\"\n",
    "    if max_time is None:\n",
    "        max_time = data.shape[1]\n",
    "\n",
    "    sensor_n = data.shape[0]\n",
    "    time_steps = (max_time - min_time) // gap_time\n",
    "    n_win = len(win_size)\n",
    "    \n",
    "    # Min-Max normalization\n",
    "    if normalize:\n",
    "        min_val = np.min(data, axis=1, keepdims=True)\n",
    "        max_val = np.max(data, axis=1, keepdims=True)\n",
    "        data = (data - min_val) / (max_val - min_val + 1e-6)\n",
    "    \n",
    "    # Initialize 4D array [time, n, n, 3]\n",
    "    result = np.zeros((time_steps, n_win, sensor_n, sensor_n))\n",
    "    \n",
    "    for w_idx, win in enumerate(win_size):\n",
    "        print(f\"Generating signature matrices with window {win}...\")\n",
    "        \n",
    "        for t_idx, t in enumerate(range(min_time, max_time, gap_time)):\n",
    "            if t < win:  # Not enough data for the window\n",
    "                mat = np.zeros((sensor_n, sensor_n))\n",
    "            else:\n",
    "                segment = data[:, t-win:t]\n",
    "                mat = np.zeros((sensor_n, sensor_n))\n",
    "                \n",
    "                for i in range(sensor_n):\n",
    "                    for j in range(i, sensor_n):\n",
    "                        mat[i,j] = np.inner(segment[i], segment[j]) / win\n",
    "                        mat[j,i] = mat[i,j]  # Symmetry\n",
    "                        \n",
    "            result[t_idx, w_idx, :, :] = mat\n",
    "        \n",
    "        if save_path:\n",
    "            np.save(f\"{save_path}/matrix_win_{win}.npy\", result[:,w_idx,:,:])\n",
    "    \n",
    "    print(\"Signature matrix generation completed!\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = [10, 30, 60]\n",
    "min_time = 0\n",
    "max_time = None\n",
    "gap_time = 10\n",
    "normalize = True\n",
    "\n",
    "signature_matrices = generate_signature_matrix(\n",
    "        data=data_val_anomalous,\n",
    "        win_size=win_size,\n",
    "        min_time=min_time,\n",
    "        max_time=max_time,\n",
    "        gap_time=gap_time,\n",
    "        normalize=normalize,\n",
    "        save_path=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c0226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def plot_signature_matrices(\n",
    "    matrix_4d: np.ndarray,\n",
    "    win_sizes: list[int] = [10, 30, 60], \n",
    "    gap_time: int = 10, \n",
    "    sample_times: list[int] = [30, 100, 200],\n",
    "    figsize: tuple = (12, 8)\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualizes multi-scale signature matrices from 4D array [time, len(win_sizes), n, n].\n",
    "    \n",
    "    Args:\n",
    "        matrix_4d: 4D array of signature matrices with shape [time, len(win_sizes), n, n]\n",
    "        win_sizes: List of window sizes corresponding to the win channels\n",
    "        gap_time: Time step between segments\n",
    "        sample_times: Times to visualize (e.g., [start, middle, end] = [max(win_sizes), len(time)/2, len(time)])\n",
    "        figsize: Figure dimensions\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the signature matrices\n",
    "    \"\"\"\n",
    "    # Entries validation\n",
    "    if matrix_4d.ndim != 4:\n",
    "        raise ValueError(\"Input matrix must be 4D array [time, len(win_sizes), n, n]\")\n",
    "    \n",
    "    n_win = len(win_sizes)\n",
    "    n_samples = len(sample_times)\n",
    "    \n",
    "    # Create grid with correct dimensions\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = GridSpec(n_win, n_samples + 1, \n",
    "                 width_ratios=[1]*n_samples + [0.05],\n",
    "                 height_ratios=[1]*n_win)\n",
    "\n",
    "    # Find global min/max for consistent color scaling\n",
    "    vmin, vmax = np.percentile(matrix_4d, [5, 95])\n",
    "\n",
    "    # Plot signature matrices\n",
    "    for i, win_size in enumerate(win_sizes):\n",
    "        for j, t in enumerate(sample_times):\n",
    "            ax = plt.subplot(gs[i, j])\n",
    "            \n",
    "            # Calculate matrix index\n",
    "            matrix_idx = t // gap_time\n",
    "            if matrix_idx >= matrix_4d.shape[0]:\n",
    "                matrix_idx = -1  # Use last available matrix\n",
    "                \n",
    "            # Get the matrix for this window size and time\n",
    "            mat = matrix_4d[matrix_idx, i, :, :]\n",
    "            \n",
    "            # Plot matrix\n",
    "            im = ax.imshow(mat, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "            ax.set_title(f\"t = {t}\\nw = {win_size}\", fontsize=10)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Add sensor labels on last row and first column\n",
    "            if i == n_win - 1:\n",
    "                ax.set_xlabel(f\"Sensor {j+1}\", fontsize=9)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f\"Scale {i+1}\", fontsize=9)\n",
    "\n",
    "    # Add colorbar\n",
    "    cax = plt.subplot(gs[:, -1])\n",
    "    plt.colorbar(im, cax=cax, label=\"Correlation\")\n",
    "    \n",
    "    plt.suptitle(\"Multi-scale Signature Matrices\", fontsize=14, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26297aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_times = [1000, 5000, 10000, 15000, 20000]\n",
    "\n",
    "plot_signature_matrices(\n",
    "    matrix_4d=signature_matrices,\n",
    "    win_sizes=win_size,\n",
    "    gap_time=gap_time,\n",
    "    sample_times=sample_times,\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38411779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "# imageio to register it\n",
    "# import imageio\n",
    "\n",
    "def create_signature_matrix_animation(\n",
    "    matrix_4d: np.ndarray,\n",
    "    window_index: int,\n",
    "    win_size: int,\n",
    "    gap_time: int = 10,\n",
    "    output_path: str = \"signature_animation.gif\",\n",
    "    fps: int = 10,\n",
    "    figsize: tuple = (8, 6),\n",
    "    dpi: int = 100\n",
    ") -> HTML:\n",
    "    \"\"\"\n",
    "    Creates an animation of signature matrices over time for a specific window.\n",
    "    \n",
    "    Args:\n",
    "        matrix_4d: 4D array of signature matrices with shape [time, len(win_sizes), n, n]\n",
    "        window_index: Index of the window to animate\n",
    "        win_size: Window size (for title)\n",
    "        gap_time: Time step between segments\n",
    "        output_path: Path to save the GIF\n",
    "        fps: Frames per second for animation\n",
    "        figsize: Figure dimensions\n",
    "        dpi: Resolution for output\n",
    "    \"\"\"\n",
    "    # Entries validation\n",
    "    if matrix_4d.ndim != 4:\n",
    "        raise ValueError(\"Input matrix must be 4D array [time, len(win_sizes), n, n]\")\n",
    "    if window_index not in [i for i in range(matrix_4d.shape[1])]:\n",
    "        raise ValueError(\"window_index must be in window range [0, 1, ..., n-1]\")\n",
    "    \n",
    "    # Extract matrices and setup figure\n",
    "    window_matrices = matrix_4d[:, window_index, :, :]\n",
    "    n_frames = window_matrices.shape[0]\n",
    "    \n",
    "    # Create figure with constrained layout\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi, constrained_layout=True)\n",
    "    \n",
    "    # Global color scale\n",
    "    vmin, vmax = np.percentile(window_matrices, [5, 95])\n",
    "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # Create initial plot and colorbar\n",
    "    im = ax.imshow(window_matrices[0], cmap='coolwarm', norm=norm)\n",
    "    ax.set_title(f\"Signature Matrix (Window size: {win_size})\\nTime: 0\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add single colorbar (outside the axes)\n",
    "    cbar = fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    cbar.set_label(\"Correlation\")\n",
    "\n",
    "    def update(frame):\n",
    "        \"\"\"Update function for animation\"\"\"\n",
    "        im.set_array(window_matrices[frame])\n",
    "        ax.set_title(f\"Signature Matrix (Window size: {win_size})\\nTime: {frame * gap_time}\")\n",
    "        return [im]\n",
    "\n",
    "    # Create animation\n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=min(n_frames, 200),  # Limit to 100 frames max for performance\n",
    "        interval=1000/fps,  # ms between frames\n",
    "        blit=True\n",
    "    )\n",
    "    \n",
    "    # Register the animation with imageio\n",
    "    # anim.save(output_path, writer='pillow', fps=fps)\n",
    "    # plt.close(fig)\n",
    "    # print(f\"Animation saved to {output_path}\")\n",
    "    \n",
    "    # Plot the animation in Jupyter Notebook\n",
    "    plt.close(fig)  # Prevent double display\n",
    "    return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09308f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_signature_matrix_animation(\n",
    "    matrix_4d=signature_matrices,\n",
    "    window_index=0,\n",
    "    win_size=10,\n",
    "    gap_time=10,\n",
    "    output_path=\"signature_animation_window10.gif\",\n",
    "    fps=5,\n",
    "    figsize=(8, 6),\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc021a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_signature_matrix_animation(\n",
    "    matrix_4d=signature_matrices,\n",
    "    window_index=1,\n",
    "    win_size=30,\n",
    "    gap_time=10,\n",
    "    output_path=\"signature_animation_window30.gif\",\n",
    "    fps=5,\n",
    "    figsize=(8, 6),\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_signature_matrix_animation(\n",
    "    matrix_4d=signature_matrices,\n",
    "    window_index=2,\n",
    "    win_size=60,\n",
    "    gap_time=10,\n",
    "    output_path=\"signature_animation_window60.gif\",\n",
    "    fps=5,\n",
    "    figsize=(8, 6),\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9f16a",
   "metadata": {},
   "source": [
    "### 3. Modèle MSCRED\n",
    "\n",
    "1. Conv_LSTM unité\n",
    "2. Conv_LSTM multicouche\n",
    "3. Attention blocs\n",
    "4. CNN encoder\n",
    "5. Conv_LSTM batché\n",
    "6. CNN decoder\n",
    "7. MSCRED total\n",
    "8. Loss adaptée\n",
    "9. MSCRED total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9826aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Torch version? \", torch.__version__)\n",
    "print(\"Cuda?          \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00854bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"GPU number : {device_count}\")\n",
    "\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No NVIDIA GPU is available for PyTorch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to force the use of one specific GPU if available and only one.\n",
    "# Indeed, the batchs must be treated in chronologic order, due to the ConvLSTM module and the time deque between batches.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        torch.cuda.set_device(0)\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device =\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Long-Short Term Memory (ConvLSTM) Cell\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional LSTM cell for processing spatio-temporal data.\n",
    "    The discrepancy between the ConvLSTM and simple LSTM lies in the use of convolutional operations instead of fully connected layers,\n",
    "    allowing the ConvLSTM to capture spatial hierarchies in the data.\n",
    "\n",
    "    Functions:\n",
    "        __init__: Initializes the ConvLSTM cell with input channels, hidden channels, and kernel size.\n",
    "        forward: Performs the forward pass of the ConvLSTM cell.\n",
    "        init_hidden: Initializes the hidden and cell states for the ConvLSTM cell.\n",
    "\n",
    "    Attributes:\n",
    "        input_channels: Number of input channels.\n",
    "        hidden_channels: Number of hidden channels in the hidden state.\n",
    "        kernel_size: Size of the convolutional kernel in the LSTM workflow.\n",
    "        num_features: Number of features in the ConvLSTM cell (4 for input, forget, cell, output gates).\n",
    "        padding: Padding size for the convolutional layers. The padding is calculated based on the kernel size.\n",
    "        Wxi, Whi, Wxf, Whf, Wxc, Whc, Wxo, Who: Convolutional layers for input, forget, and output gates regarding the input and the hidden states.\n",
    "        Wci, Wcf, Wco: Cell state weights for input, forget, and output gates regarding the cell state(initialized to None).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_channels: int, \n",
    "            hidden_channels: int, \n",
    "            kernel_size: int | tuple\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_channels % 2 == 0\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        # self.num_features = 4\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        self.Wxi = nn.Conv2d(in_channels=self.input_channels,  out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=True )\n",
    "        self.Whi = nn.Conv2d(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(in_channels=self.input_channels,  out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=True )\n",
    "        self.Whf = nn.Conv2d(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(in_channels=self.input_channels,  out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=True )\n",
    "        self.Whc = nn.Conv2d(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(in_channels=self.input_channels,  out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=True)\n",
    "        self.Who = nn.Conv2d(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.Tensor, \n",
    "            h: torch.Tensor, \n",
    "            c: torch.Tensor\n",
    "            ) -> tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the ConvLSTM cell.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_channels, height, width)\n",
    "            h: Hidden state tensor of shape (batch_size, hidden_channels, height, width)\n",
    "            c: Cell state tensor of shape (batch_size, hidden_channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of updated hidden state and cell state tensors, shape (batch_size, hidden_channels, height, width)\n",
    "        \"\"\"\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(\n",
    "            self,\n",
    "            batch_size: int, \n",
    "            hidden: int, \n",
    "            shape: tuple[int], \n",
    "            device: str\n",
    "            ) -> tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Initializes weight for the convLSTM cell. In two parts:\n",
    "            - The weight matrices for the Input, Forget, and Output gates regarding the cell state (Wci, Wcf, Wco). \n",
    "              These are initialized to zero tensors, in order to limit the influence of the cell state (and its gradient) in first iterations.\n",
    "            \n",
    "            - The very first hidden state and cell state (h and c). h and c represent respectively the short and long-term memory.\n",
    "              Those are also initialized to zero tensors, because no history is available at the first time step.\n",
    "        \n",
    "        This is done only once, at the first step of the forward pass.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Size of the batch\n",
    "            hidden: Number of hidden channels\n",
    "            shape: Shape of the input (height, width)\n",
    "            device: Device to place the tensors on\n",
    "\n",
    "        Returns:\n",
    "            Tuple of initialized hidden state and cell state tensors.\n",
    "        \"\"\"\n",
    "        if self.Wci is None:\n",
    "            self.Wci = torch.zeros(1, hidden, shape[0], shape[1], device=device)\n",
    "            self.Wcf = torch.zeros(1, hidden, shape[0], shape[1], device=device)\n",
    "            self.Wco = torch.zeros(1, hidden, shape[0], shape[1], device=device)\n",
    "        else:\n",
    "            assert shape[0] == self.Wci.shape[2], 'Input Height Mismatched!'\n",
    "            assert shape[1] == self.Wci.shape[3], 'Input Width Mismatched!'\n",
    "        return (torch.zeros(batch_size, hidden, shape[0], shape[1], device=device),\n",
    "                torch.zeros(batch_size, hidden, shape[0], shape[1], device=device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    \"\"\"\n",
    "    Convolutional LSTM model for processing spatio-temporal data.\n",
    "    A ConvLSTM model is a sequence of ConvLSTM cells that processes the input tensor over multiple steps.\n",
    "    If len(hidden_channels) == 1, it behaves like a standard ConvLSTM cell alone.\n",
    "\n",
    "    Functions:\n",
    "        __init__: Initializes the ConvLSTM model with input channels, hidden channels, kernel size, step, and effective step.\n",
    "        reset_hidden_state: Resets the hidden states h and c of the ConvLSTM model to zeros.\n",
    "        forward: Performs the forward pass of the ConvLSTM model.\n",
    "\n",
    "    Attributes:\n",
    "        input_channels: Number of channels for the first input feature map. Correspond to the inputs of ConvLSTMCell.\n",
    "        hidden_channels: List of hidden channels for each layer. Correspond to the outputs of ConvLSTMCell.\n",
    "        kernel_size: Size of the convolutional kernel in Convolutional layers of the ConvLSTM cell.\n",
    "        num_layers: Number of ConvLSTM layers.\n",
    "        step: Number of steps to process in the forward pass.\n",
    "        effective_step: List of effective steps to record outputs.\n",
    "        _all_layers: List of all ConvLSTM cells in the model. Each are callable objects that refers to the corresponding ConvLSTM cell.\n",
    "        next_hidden_state: List of next hidden states for each layer. Will be use for the next window pass on the ConvLSTM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_channels: int, \n",
    "            hidden_channels: list[int], \n",
    "            kernel_size: int | tuple, \n",
    "            step: int = 1, \n",
    "            effective_step: list[int] = [0, 1]\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        assert step >= 1, \"n_step must be at least 1 to find temporal dependencies and to register the next hidden state.\"\n",
    "\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "\n",
    "        self._all_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            cell = ConvLSTMCell(\n",
    "                input_channels=self.input_channels[i],\n",
    "                hidden_channels=self.hidden_channels[i],\n",
    "                kernel_size=self.kernel_size\n",
    "            )\n",
    "            self._all_layers.append(cell)\n",
    "\n",
    "        self.next_hidden_state = [None] * self.num_layers\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        \"\"\"\n",
    "        Reset the hidden state of the ConvLSTM model.\n",
    "        For instance, when we start a new epoch in the training, we want to reset the hidden state.\n",
    "        \"\"\"\n",
    "        self.next_hidden_state = [None] * self.num_layers\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input: torch.Tensor,\n",
    "            ) -> tuple[list[torch.Tensor], tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the ConvLSTM model.\n",
    "        A ConvLSTM model is a sequence of ConvLSTM cells that processes the input tensor over multiple steps.\n",
    "        For each step, the input tensor is passed through all layers.\n",
    "        Each layer is a ConvLSTM cell that takes the output of the previous layer as input.\n",
    "\n",
    "        Args:\n",
    "            input: Input tensor of shape (self.step, input_channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - List of output tensors for each effective step: [h_0, h_1, ..., h_step]\n",
    "                - Tuple of the last hidden state and cell state tensors (h_lastlayer_laststep, c_lastlayer_laststep)\n",
    "        \n",
    "        Nota bene:\n",
    "            We initialize the internal state (hidden and cell states) for each layer in the first step.\n",
    "            And that for each time window (of size self.step). Whether the model is in training or inference mode, the internal states are reset.\n",
    "            Indeed, as the time window are overlapping, the internal state of the\n",
    "        \"\"\"\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "\n",
    "        input = input.unsqueeze(1) if input.dim() == 4 else input  # Ensure input is 5D: (step, 1, channels, height, width)\n",
    "        \n",
    "        for step_idx in range(self.step):\n",
    "            x = input[step_idx]\n",
    "\n",
    "            for layer_idx, cell in enumerate(self._all_layers):\n",
    "                if step_idx == 0:\n",
    "                    if self.next_hidden_state[layer_idx] is None:\n",
    "                        bsize, _, height, width = x.shape\n",
    "                        h, c = cell.init_hidden(batch_size=bsize, hidden=self.hidden_channels[layer_idx], shape=(height, width), device=x.device)\n",
    "                    else:\n",
    "                        # if next_hidden_state is provided, use it\n",
    "                        h, c = self.next_hidden_state[layer_idx]\n",
    "                    internal_state.append((h, c))\n",
    "                                   \n",
    "                # do forward\n",
    "                h, c = internal_state[layer_idx]\n",
    "                new_h, new_c = cell(x, h, c)\n",
    "                internal_state[layer_idx] = (new_h, new_c)\n",
    "                # output of the current layer is the input for the next layer\n",
    "                x = new_h\n",
    "\n",
    "                ### if step_idx == 1 and not self.training: # <------- Uncomment this line to use persistant hidden states only during inference\n",
    "                if step_idx == 1:      \n",
    "                    # register the hidden state for the next time window\n",
    "                    h_save, c_save = new_h.clone().detach(), new_c.clone().detach()\n",
    "                    self.next_hidden_state[layer_idx] = (h_save, c_save)\n",
    "\n",
    "            # only record effective steps of the last layer\n",
    "            if step_idx in self.effective_step:\n",
    "                outputs.append(new_h)\n",
    "\n",
    "        return outputs, (new_h, new_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f175ce",
   "metadata": {},
   "source": [
    "#### Test ConvLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_channels = 3\n",
    "height, width = 32, 32\n",
    "kernel_size = 3\n",
    "hidden_channels = 64\n",
    "x = torch.randn(batch_size, input_channels, height, width).to(device)\n",
    "\n",
    "convlstm_cell = ConvLSTMCell(\n",
    "    input_channels=input_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    kernel_size=kernel_size\n",
    ").to(device)\n",
    "h, c = convlstm_cell.init_hidden(batch_size, hidden_channels, (height, width), device)\n",
    "new_h, new_c = convlstm_cell(x, h, c)\n",
    "\n",
    "print(\"Shape of input x:\", x.shape)\n",
    "print(\"\\nShape of first hidden state h:    \", h.shape)\n",
    "print(\"Shape of first cell   state c:    \", c.shape)\n",
    "print(\"Shape of new   hidden state new_h:\", new_h.shape)\n",
    "print(\"Shape of new   cell   state new_c:\", new_c. shape)\n",
    "assert new_h.shape == (batch_size, hidden_channels, height, width)\n",
    "assert new_c.shape == new_h.shape\n",
    "print(\"\\nAll shapes are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41604c28",
   "metadata": {},
   "source": [
    "#### Test ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba030d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_channels = 3\n",
    "hidden_channels = [32, 64, 128]\n",
    "height, width = 32, 32\n",
    "kernel_size = 3\n",
    "steps = 5\n",
    "effective_step = [0, 1, 2, 3, 4]\n",
    "\n",
    "x = torch.randn(steps, input_channels, height, width).to(device)\n",
    "\n",
    "convlstm = ConvLSTM(\n",
    "    input_channels=input_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    step=steps,\n",
    "    effective_step=effective_step\n",
    ").to(device)\n",
    "outputs, (final_h, final_c) = convlstm(x)\n",
    "\n",
    "\n",
    "print(\"Shape of input x:\", x.shape)\n",
    "print(\"Number of ConvLSTM layers:\", len(convlstm.hidden_channels))\n",
    "print(\"Number of ConvLSTM layers 2:\", convlstm.num_layers)\n",
    "print(\"Number of recorded outputs:\", len(outputs))\n",
    "print(\"Effective steps:\", convlstm.effective_step)\n",
    "print(\"Layer type:\", type(convlstm._all_layers[0]))\n",
    "\n",
    "print(\"\\nShape of output tensors:\")\n",
    "for i, step in enumerate(convlstm.effective_step):\n",
    "    print(f\"Output of step {step}:\")\n",
    "    print(\"Shape:\", outputs[i].shape)\n",
    "assert outputs[i].shape == (batch_size, hidden_channels[-1], height, width)\n",
    "\n",
    "print(\"\\nShape of last hidden state final_h:\", final_h.shape)\n",
    "print(\"Shape of last cell state final_c:\", final_c.shape)\n",
    "assert final_h.shape == (batch_size, hidden_channels[-1], height, width)\n",
    "assert final_c.shape == final_h.shape\n",
    "\n",
    "print(\"\\nAll shapes are correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4728cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention mechanism for ConvLSTM output\n",
    "def attention(\n",
    "        ConvLstm_out: torch.Tensor,\n",
    "        rescale_factor: float = 5.0\n",
    "        ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Attention mechanism for ConvLSTM output (Vectorized)\n",
    "    This implementation build a causal \"by construction\" attention mechanism. \"By construction\" because we only give passed informations.\n",
    "\n",
    "    Args:\n",
    "        ConvLstm_out: Tensor output from the ConvLSTM layer. Shape (n_effective_timestep, channels, height, width)\n",
    "        n: Number of steps to consider for attention weighting.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The output after applying the attention mechanism. Shape (channels, height, width)\n",
    "\n",
    "    Nota bene:\n",
    "        The rescale_factor is used to normalize the similarity scores after the dot product of the \n",
    "        hidden states and before the softmax function.\n",
    "        The value of 5 is empirically determined.\n",
    "        This value is a compromise between producing uniform attention weights (large rescale_factor = all weights are the same order) \n",
    "        and producing a sharp distribution in weights (small rescale_factor = one weight dominates) with the softmax function.\n",
    "    \n",
    "    Post Scriptum: \n",
    "        In the future, we could replace this attention mechanism with a more sophisticated one, such as a query-key attention mechanism.\n",
    "    \"\"\"\n",
    "    assert rescale_factor > 0, \"Rescale factor must be positive and non zero\"\n",
    "\n",
    "    last_step = ConvLstm_out[-1]\n",
    "    similarities = torch.einsum('tchw,chw->t', ConvLstm_out, last_step) / rescale_factor\n",
    "    attention_weights = torch.softmax(similarities, dim=0)\n",
    "    ConvLstm_out_weighted = torch.einsum('t,tchw->chw', attention_weights, ConvLstm_out)\n",
    "    return ConvLstm_out_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984de6a",
   "metadata": {},
   "source": [
    "#### Test attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_effective_timestep = 5\n",
    "batch_size = 1\n",
    "channels = 64\n",
    "height = 32\n",
    "width = 32\n",
    "\n",
    "conv_lstm_out = torch.randn(n_effective_timestep, channels, height, width)\n",
    "print(\"Shape of ConvLSTM output tensor:\", conv_lstm_out.shape)\n",
    "\n",
    "attention_output = attention(conv_lstm_out)\n",
    "\n",
    "print(\"Attention output shape:\", attention_output.shape)\n",
    "assert attention_output.shape == (channels, height, width), f\"Expected shape: {(channels, height, width)}, obtained: {attention_output.shape}\"\n",
    "assert not torch.isnan(attention_output).any(), \"Result contains NaN values\"\n",
    "assert not torch.isinf(attention_output).any(), \"Result contains infinite values\"\n",
    "\n",
    "for rescale in [0.1, 1.0, 5.0, 10.0, 100.0]:\n",
    "    try:\n",
    "        output = attention(conv_lstm_out, rescale_factor=rescale)\n",
    "        assert output.shape == (channels, height, width)\n",
    "    except Exception as e:\n",
    "        assert False, f\"Error with rescale_factor={rescale}: {str(e)}\"\n",
    "\n",
    "print(\"\\nAll tests passed with success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder-Decoder modules\n",
    "class CnnEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Encoder module\n",
    "\n",
    "    This module applies a series of convolutional layers to the input data.\n",
    "    Shapes: (batch_size, 32,  height,    width)    after conv1 (stride 1)\n",
    "            (batch_size, 64,  height//2, width//2) after conv2 (stride 2)\n",
    "            (batch_size, 128, height//4, width//4) after conv3 (stride 2)\n",
    "            (batch_size, 256, height//8, width//8) after conv4 (stride 2)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels_encoder: int = 3, \n",
    "            deep_channel_sizes: list[int] = [32, 64, 128, 256]\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        We have chosen SELU() activation function for its self-normalizing properties.\n",
    "\n",
    "        Advices:\n",
    "            - in_channels = [in_channels_encoder, 32, 64, 128]\n",
    "            - out_channels = [32, 64, 128, 256]\n",
    "            - kernel_sizes = [3, 3, 2, 2]\n",
    "            - stride must be (2, 2) for the first three layers, (1, 1) for the last layer (no dimension reduction)\n",
    "            - padding must be (0, 0) for the last layer (no operations after the last layer), (1, 1) for the others\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels_encoder, \n",
    "                      out_channels=deep_channel_sizes[0], \n",
    "                      kernel_size=3, stride=(1, 1), padding=1),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=deep_channel_sizes[0], \n",
    "                      out_channels=deep_channel_sizes[1], \n",
    "                      kernel_size=3, stride=(2, 2), padding=1),\n",
    "            nn.SELU()\n",
    "        )    \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=deep_channel_sizes[1], \n",
    "                      out_channels=deep_channel_sizes[2], \n",
    "                      kernel_size=2, stride=(2, 2), padding=0),\n",
    "            nn.SELU()\n",
    "        )   \n",
    "        # self.conv4 = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=deep_channel_sizes[2], \n",
    "        #               out_channels=deep_channel_sizes[3], \n",
    "        #               kernel_size=2, stride=(2, 2), padding=0),\n",
    "        #     nn.SELU()\n",
    "        # )\n",
    "    def forward(self, X: torch.Tensor) -> tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for the CNN encoder\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor]: Output tensors from each convolutional layer\n",
    "        \"\"\"\n",
    "        conv1_out = self.conv1(X)\n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv3_out = self.conv3(conv2_out)\n",
    "        # conv4_out = self.conv4(conv3_out)\n",
    "        return conv1_out, conv2_out, conv3_out#, conv4_out\n",
    "\n",
    "class Conv_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional LSTM module\n",
    "    This module applies a series of ConvLSTM layers to the output of CnnEncoder.\n",
    "\n",
    "    Functions:\n",
    "        - __init__: Initializes the Conv_LSTM module.\n",
    "        - forward: Defines the forward pass for the Conv_LSTM module.\n",
    "        - _init_queue: Initializes the internal queues for the Conv_LSTM module.\n",
    "        - _update_queues: Updates the internal queues with the latest CnnEncoder output.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            deep_channel_sizes: list[int] = [32, 64, 128, 256],\n",
    "            num_layers: int = 1,\n",
    "            n_timesteps: int = 5, \n",
    "            n_effective_timesteps: list = [1, 2, 3, 4]\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1_lstm = ConvLSTM(input_channels=deep_channel_sizes[0], \n",
    "                                   hidden_channels=[deep_channel_sizes[0]]*num_layers, \n",
    "                                   kernel_size=3, \n",
    "                                   step=n_timesteps, effective_step=n_effective_timesteps)\n",
    "        self.conv2_lstm = ConvLSTM(input_channels=deep_channel_sizes[1], \n",
    "                                   hidden_channels=[deep_channel_sizes[1]]*num_layers, \n",
    "                                   kernel_size=3, \n",
    "                                   step=n_timesteps, effective_step=n_effective_timesteps)\n",
    "        self.conv3_lstm = ConvLSTM(input_channels=deep_channel_sizes[2], \n",
    "                                   hidden_channels=[deep_channel_sizes[2]]*num_layers, \n",
    "                                   kernel_size=3, \n",
    "                                   step=n_timesteps, effective_step=n_effective_timesteps)\n",
    "        # self.conv4_lstm = ConvLSTM(input_channels=deep_channel_sizes[3], \n",
    "        #                            hidden_channels=[deep_channel_sizes[3]]*num_layers, \n",
    "        #                            kernel_size=3, \n",
    "        #                            step=n_timesteps, effective_step=n_effective_timesteps)\n",
    "\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.queue1 = deque(maxlen=n_timesteps)\n",
    "        self.queue2 = deque(maxlen=n_timesteps)\n",
    "        self.queue3 = deque(maxlen=n_timesteps)\n",
    "        # self.queue4 = deque(maxlen=n_timestep)\n",
    "\n",
    "    def _init_queue(\n",
    "            self, \n",
    "            queue: deque, \n",
    "            in_shape: tuple, \n",
    "            steps: int,\n",
    "            device: str\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the queues with zeros for the first `steps - 1` timesteps.\n",
    "\n",
    "        Args:\n",
    "            queue (deque): Queue to initialize\n",
    "            in_shape (tuple): Shape of the input tensor\n",
    "            steps (int): Number of timesteps\n",
    "            device (str): Device to place the tensors on\n",
    "\n",
    "        Nota bene:\n",
    "            The zeros tensors are created with `requires_grad=False` to avoid unnecessary gradient computations.\n",
    "        \"\"\"\n",
    "        queue.clear()\n",
    "        for _ in range(steps):\n",
    "            # resize to be shape [steps, C, H, W]\n",
    "            queue.append(torch.zeros(in_shape[1:], device=device, requires_grad=False))\n",
    "\n",
    "    def _update_queues(self, x_conv1_out: torch.Tensor, x_conv2_out: torch.Tensor, x_conv3_out: torch.Tensor, x_conv4_out: torch.Tensor = 0) -> None:\n",
    "        \"\"\"\n",
    "        Update the queues with the latest CnnEncoder output with this scheme:\n",
    "            - Theoretically, in the last queue, all tensors are detached form autograd, except for the most recent one.\n",
    "            - We pop the recent tensors from the queues and detach them from the computation graph.\n",
    "            - We add this detached tensor back to the queue.\n",
    "            - We add the new tensor to the queue.\n",
    "            - This new tensor is attached to the computation graph.\n",
    "            - Thanks to maxlen deque's behavior, we ensure that the queue always contains the n_timesteps most recent tensors.\n",
    "\n",
    "        Args:\n",
    "            x_convi_out (torch.Tensor): Output from the i_th CnnEncoder layer.\n",
    "        \"\"\"\n",
    "        # resize to be shape [steps, C, H, W]\n",
    "        assert len(self.queue1) == self.n_timesteps, \"Queue length must match n_timesteps\"\n",
    "        assert len(self.queue2) == self.n_timesteps, \"Queue length must match n_timesteps\"\n",
    "        assert len(self.queue3) == self.n_timesteps, \"Queue length must match n_timesteps\"\n",
    "        # assert len(self.queue4) == self.n_timesteps, \"Queue length must match n_timesteps\"\n",
    "\n",
    "        old_grad_tensor1 = self.queue1.pop()\n",
    "        old_grad_tensor2 = self.queue2.pop()\n",
    "        old_grad_tensor3 = self.queue3.pop()\n",
    "        # old_grad_tensor4 = self.queue4.pop()\n",
    "        \n",
    "        old_grad_tensor1 = old_grad_tensor1.detach()\n",
    "        old_grad_tensor2 = old_grad_tensor2.detach()\n",
    "        old_grad_tensor3 = old_grad_tensor3.detach()\n",
    "        # old_grad_tensor4 = old_grad_tensor4.detach()\n",
    "\n",
    "        self.queue1.append(old_grad_tensor1)\n",
    "        self.queue2.append(old_grad_tensor2)\n",
    "        self.queue3.append(old_grad_tensor3)\n",
    "        # self.queue4.append(old_tensor4)\n",
    "\n",
    "        self.queue1.append(x_conv1_out)\n",
    "        self.queue2.append(x_conv2_out)\n",
    "        self.queue3.append(x_conv3_out)\n",
    "        # self.queue4.append(x_conv4_out)\n",
    "\n",
    "    def forward(self, conv1_out: torch.Tensor, conv2_out: torch.Tensor, conv3_out: torch.Tensor, conv4_out: torch.Tensor = 0) -> tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        ConvLSTM forward pass\n",
    "        convi_lstm_out is a tuple (outputs, (x, new_c))\n",
    "            - output: the registered hidden state of effective_step list\n",
    "            - (x, new_c): the hidden state and cell state for the last step\n",
    "        Exemple of shape for conv2_lstm_out:\n",
    "            output.shape = (batch_size, 64, height/2, width/2)\n",
    "            x.shape = new_c.shape = (batch_size, 64, height/2, width/2)\n",
    "        So convi_lstm_out[0][0] is the first element of output.\n",
    "\n",
    "        Args:\n",
    "            convi_out (torch.Tensor): Output from the ith convolutional layer\n",
    "            conv4_out (torch.Tensor, optional): Output from the fourth convolutional layer\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor]: Output tensors from each ConvLSTM layer\n",
    "        \"\"\"\n",
    "        for queue, convX in [(self.queue1, conv1_out), (self.queue2, conv2_out), (self.queue3, conv3_out)]:\n",
    "            if bool(queue) is False:\n",
    "                self._init_queue(queue, convX.shape, self.n_timesteps, device=convX.device)\n",
    "\n",
    "        conv1_lstm_out = torch.zeros_like(conv1_out).to(device=conv1_out.device)\n",
    "        conv2_lstm_out = torch.zeros_like(conv2_out).to(device=conv2_out.device)\n",
    "        conv3_lstm_out = torch.zeros_like(conv3_out).to(device=conv3_out.device)\n",
    "        # conv4_lstm_out = torch.zeros_like(conv4_out).to(device=conv4_out.device)\n",
    "\n",
    "        for batch_idx, (x_conv1, x_conv2, x_conv3) in enumerate(zip(conv1_out, conv2_out, conv3_out)):\n",
    "            x_conv1 = x_conv1.to(conv1_out.device)\n",
    "            x_conv2 = x_conv2.to(conv2_out.device)\n",
    "            x_conv3 = x_conv3.to(conv3_out.device)\n",
    "            # x_conv4 = x_conv4.to(conv4_out.device)\n",
    "\n",
    "            self._update_queues(x_conv1, x_conv2, x_conv3)\n",
    "\n",
    "            x_conv1_lstm_in = torch.stack(tuple(self.queue1), dim=0)\n",
    "            x_conv2_lstm_in = torch.stack(tuple(self.queue2), dim=0)\n",
    "            x_conv3_lstm_in = torch.stack(tuple(self.queue3), dim=0)\n",
    "            # x_conv4_lstm_in = torch.stack(tuple(self.queue4), dim=0)\n",
    "\n",
    "            x_conv1_lstm_out = self.conv1_lstm(x_conv1_lstm_in)\n",
    "            x_conv1_lstm_out = attention(ConvLstm_out=torch.cat(x_conv1_lstm_out[0], dim=0))\n",
    "            x_conv2_lstm_out = self.conv2_lstm(x_conv2_lstm_in)\n",
    "            x_conv2_lstm_out = attention(ConvLstm_out=torch.cat(x_conv2_lstm_out[0], dim=0))\n",
    "            x_conv3_lstm_out = self.conv3_lstm(x_conv3_lstm_in)\n",
    "            x_conv3_lstm_out = attention(ConvLstm_out=torch.cat(x_conv3_lstm_out[0], dim=0))\n",
    "            # x_conv4_lstm_out = self.conv4_lstm(x_conv4_lstm_in)\n",
    "            # x_conv4_lstm_out = attention(ConvLstm_out=torch.cat(x_conv4_lstm_out[0], dim=0))\n",
    "            \n",
    "            conv1_lstm_out[batch_idx] = x_conv1_lstm_out\n",
    "            conv2_lstm_out[batch_idx] = x_conv2_lstm_out\n",
    "            conv3_lstm_out[batch_idx] = x_conv3_lstm_out\n",
    "            # conv4_lstm_out[batch_idx] = x_conv4_lstm_out\n",
    "\n",
    "        return conv1_lstm_out, conv2_lstm_out, conv3_lstm_out, #conv4_lstm_out\n",
    "\n",
    "class CnnDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Decoder module\n",
    "\n",
    "    This module applies a series of transposed convolutional layers to the input data.\n",
    "    Shapes: (batch_size, 128, height*2, width*2) after deconv4 (stride 2)\n",
    "            (batch_size, 256, height*2, width*2) after first concatenation\n",
    "            (batch_size, 64,  height*4, width*4) after deconv3 (stride 2)\n",
    "            (batch_size, 128, height*4, width*4) after second concatenation\n",
    "            (batch_size, 32,  height*8, width*8) after deconv2 (stride 2)\n",
    "            (batch_size, 64,  height*8, width*8) after first concatenation\n",
    "            (batch_size, 3,   height*8, width*8) after deconv1 (stride 1)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels_encoder: int = 3,\n",
    "            deep_channel_sizes: list[int] = [32, 64, 128, 256]\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        We have chosen SELU() activation function for its self-normalizing properties.\n",
    "        \n",
    "        Advices:\n",
    "            - in_channels = hidden_channels of the corresponding ConvLSTM layer + concatenation\n",
    "                + if conv4_lstm_out is not None, [256, 256, 128, 64]\n",
    "                + if conv4_lstm_out is None, [128, 128, 64]\n",
    "            - out_channels\n",
    "                + if conv4_lstm_out is not None, [128, 64, 32, encoder_in_channels]\n",
    "                + if conv4_lstm_out is None, [64, 32, encoder_in_channels]\n",
    "            - kernel_sizes = [2, 2, 3, 3]\n",
    "            - stride must be (2, 2) for the first three layers, (1, 1) for the last layer (no dimension reduction)\n",
    "            - padding must be (0, 0) for the first layer, (1, 1) for the others\n",
    "            - output_padding must be (0, 0) for the first and last layer, (1, 1) for the others\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        deep_channel_sizes_decode = deep_channel_sizes[::-1]\n",
    "\n",
    "        # self.deconv4 = nn.Sequential(\n",
    "        #     nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2, padding=0, output_padding=0),\n",
    "        #     nn.SELU()\n",
    "        # )\n",
    "        ### If conv4_lstm_out is not None, put in_channels(self.deconv3)=256, else 128\n",
    "        self.deconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=deep_channel_sizes_decode[0], \n",
    "                               out_channels=deep_channel_sizes_decode[1], \n",
    "                               kernel_size=2, stride=2, \n",
    "                               padding=0, output_padding=0),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=deep_channel_sizes_decode[1]*2, \n",
    "                               out_channels=deep_channel_sizes_decode[2], \n",
    "                               kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=deep_channel_sizes_decode[2]*2, \n",
    "                               out_channels=in_channels_encoder, \n",
    "                               kernel_size=3, stride=1, \n",
    "                               padding=1, output_padding=0),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, conv1_lstm_out: torch.Tensor, conv2_lstm_out: torch.Tensor, conv3_lstm_out: torch.Tensor, conv4_lstm_out: torch.Tensor = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the CNN decoder\n",
    "\n",
    "        Args:\n",
    "            conv1_lstm_out (torch.Tensor): Output from the first LSTM layer\n",
    "            conv2_lstm_out (torch.Tensor): Output from the second LSTM layer\n",
    "            conv3_lstm_out (torch.Tensor): Output from the third LSTM layer\n",
    "            conv4_lstm_out (torch.Tensor, optional): Output from the fourth LSTM layer\n",
    "        Shapes:\n",
    "            - conv1_lstm_out: (batch_size, 32, height, width)\n",
    "            - conv2_lstm_out: (batch_size, 64, height, width)\n",
    "            - conv3_lstm_out: (batch_size, 128, height, width)\n",
    "            - conv4_lstm_out: (batch_size, 256, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor from the decoder. Shape (batch_size, in_channels_encoder, height, width)\n",
    "        \"\"\"\n",
    "        # deconv4 = self.deconv4(conv4_lstm_out)\n",
    "        # deconv4_concat = torch.cat((deconv4, conv3_lstm_out), dim = 1)\n",
    "        deconv3 = self.deconv3(conv3_lstm_out)      #deconv3 = self.deconv3(deconv4_concat)\n",
    "        deconv3_concat = torch.cat((deconv3, conv2_lstm_out), dim = 1)\n",
    "        deconv2 = self.deconv2(deconv3_concat)\n",
    "        deconv2_concat = torch.cat((deconv2, conv1_lstm_out), dim = 1)\n",
    "        deconv1 = self.deconv1(deconv2_concat)\n",
    "        return deconv1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245baf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_LSTM_2_a_revoir(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional LSTM module vectorialisée\n",
    "    Utilise des buffers PyTorch pour maintenir l'état temporel entre les batches\n",
    "\n",
    "    PAS ENCORE FINI, à reprendre\n",
    "    Il faudra alors modifier l'Attention pour prendre en compte le batch_size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_timesteps: int = 5, n_effective_timesteps: list = [1, 2, 3, 4]) -> None:\n",
    "        super().__init__()\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_effective_timesteps = n_effective_timesteps\n",
    "        \n",
    "        # ConvLSTM layers\n",
    "        self.conv1_lstm = ConvLSTM(input_channels=32, hidden_channels=[32], \n",
    "                                   kernel_size=3, step=n_timesteps, effective_step=n_effective_timesteps)\n",
    "        self.conv2_lstm = ConvLSTM(input_channels=64, hidden_channels=[64], \n",
    "                                   kernel_size=3, step=n_timesteps, effective_step=n_effective_timesteps)\n",
    "        self.conv3_lstm = ConvLSTM(input_channels=128, hidden_channels=[128], \n",
    "                                   kernel_size=3, step=n_timesteps, effective_step=n_effective_timesteps)\n",
    "        \n",
    "        # Buffers pour stocker l'état temporel (seront initialisés lors du premier forward)\n",
    "        self.register_buffer('queue1', None)\n",
    "        self.register_buffer('queue2', None)\n",
    "        self.register_buffer('queue3', None)\n",
    "        self.register_buffer('ptr', torch.tensor(0, dtype=torch.long))\n",
    "        \n",
    "    def _init_queues(self, conv1_shape: torch.Size, conv2_shape: torch.Size, conv3_shape: torch.Size) -> None:\n",
    "        device = self.ptr.device\n",
    "        \n",
    "        # Initialisation des queues avec des zéros\n",
    "        self.queue1 = torch.zeros(self.n_timesteps, *conv1_shape[1:], device=device)\n",
    "        self.queue2 = torch.zeros(self.n_timesteps, *conv2_shape[1:], device=device)\n",
    "        self.queue3 = torch.zeros(self.n_timesteps, *conv3_shape[1:], device=device)\n",
    "        \n",
    "    def _update_queues(self, conv1_out: torch.Tensor, conv2_out: torch.Tensor, conv3_out: torch.Tensor) -> None:\n",
    "        batch_size = conv1_out.size(0)\n",
    "        \n",
    "        # Détacher les anciennes valeurs (sauf la position actuelle)\n",
    "        with torch.no_grad():\n",
    "            # Mise à jour circulaire\n",
    "            self.queue1[self.ptr] = conv1_out.detach().mean(dim=0)  # Moyenne sur le batch\n",
    "            self.queue2[self.ptr] = conv2_out.detach().mean(dim=0)\n",
    "            self.queue3[self.ptr] = conv3_out.detach().mean(dim=0)\n",
    "        \n",
    "        # Mise à jour du pointeur\n",
    "        self.ptr = (self.ptr + 1) % self.n_timesteps\n",
    "    \n",
    "    def _get_lstm_inputs(self, conv1_out: torch.Tensor, conv2_out: torch.Tensor, conv3_out: torch.Tensor) -> tuple:\n",
    "        batch_size = conv1_out.size(0)\n",
    "        \n",
    "        # Préparer les entrées LSTM avec connexion des gradients pour les nouvelles valeurs\n",
    "        lstm_input1 = self.queue1.clone().detach()\n",
    "        lstm_input2 = self.queue2.clone().detach()\n",
    "        lstm_input3 = self.queue3.clone().detach()\n",
    "        \n",
    "        # Remplacer la position la plus ancienne par les nouvelles valeurs (avec gradient)\n",
    "        oldest_ptr = (self.ptr) % self.n_timesteps  # La plus ancienne valeur\n",
    "        lstm_input1[oldest_ptr] = conv1_out.mean(dim=0)  # Moyenne sur le batch\n",
    "        lstm_input2[oldest_ptr] = conv2_out.mean(dim=0)\n",
    "        lstm_input3[oldest_ptr] = conv3_out.mean(dim=0)\n",
    "        \n",
    "        # Étendre la dimension batch pour correspondre aux attentes de ConvLSTM\n",
    "        lstm_input1 = lstm_input1.unsqueeze(1).expand(-1, batch_size, -1, -1, -1)\n",
    "        lstm_input2 = lstm_input2.unsqueeze(1).expand(-1, batch_size, -1, -1, -1)\n",
    "        lstm_input3 = lstm_input3.unsqueeze(1).expand(-1, batch_size, -1, -1, -1)\n",
    "        \n",
    "        return lstm_input1, lstm_input2, lstm_input3\n",
    "\n",
    "    def forward(self, conv1_out: torch.Tensor, conv2_out: torch.Tensor, conv3_out: torch.Tensor) -> tuple:\n",
    "        # Initialisation des queues si nécessaire\n",
    "        if self.queue1 is None:\n",
    "            self._init_queues(conv1_out.shape, conv2_out.shape, conv3_out.shape)\n",
    "        \n",
    "        # Mise à jour des queues avec les nouvelles valeurs (détachées)\n",
    "        self._update_queues(conv1_out, conv2_out, conv3_out)\n",
    "        \n",
    "        # Préparation des entrées LSTM\n",
    "        lstm_input1, lstm_input2, lstm_input3 = self._get_lstm_inputs(conv1_out, conv2_out, conv3_out)\n",
    "        \n",
    "        # Passage through ConvLSTM\n",
    "        conv1_lstm_out = self.conv1_lstm(lstm_input1)\n",
    "        conv2_lstm_out = self.conv2_lstm(lstm_input2)\n",
    "        conv3_lstm_out = self.conv3_lstm(lstm_input3)\n",
    "        \n",
    "        # Application de l'attention\n",
    "        conv1_lstm_out = attention(ConvLstm_out=torch.cat(conv1_lstm_out[0], dim=0))\n",
    "        conv2_lstm_out = attention(ConvLstm_out=torch.cat(conv2_lstm_out[0], dim=0))\n",
    "        conv3_lstm_out = attention(ConvLstm_out=torch.cat(conv3_lstm_out[0], dim=0))\n",
    "        \n",
    "        # Étendre les résultats pour correspondre à la taille du batch\n",
    "        conv1_lstm_out = conv1_lstm_out.unsqueeze(0).expand(conv1_out.size(0), -1, -1, -1)\n",
    "        conv2_lstm_out = conv2_lstm_out.unsqueeze(0).expand(conv2_out.size(0), -1, -1, -1)\n",
    "        conv3_lstm_out = conv3_lstm_out.unsqueeze(0).expand(conv3_out.size(0), -1, -1, -1)\n",
    "        \n",
    "        return conv1_lstm_out, conv2_lstm_out, conv3_lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae17b0",
   "metadata": {},
   "source": [
    "#### Test for CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e206ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "height = 32\n",
    "width = 32\n",
    "in_channels = 3\n",
    "deep_channels = [32, 64, 128]\n",
    "\n",
    "X = torch.randn(batch_size, in_channels, height, width)\n",
    "\n",
    "encoder = CnnEncoder(in_channels_encoder=in_channels, deep_channel_sizes=deep_channels)\n",
    "conv1_out, conv2_out, conv3_out = encoder(X)\n",
    "\n",
    "print(\"Shape of input X:\", X.shape)\n",
    "\n",
    "print(\"\\nConv1_out shape (must be the same because stride 1):\", conv1_out.shape)\n",
    "assert conv1_out.shape == (batch_size, 32, height, width), f\"conv1_out shape expected: {(batch_size, 32, height, width)}, obtained: {conv1_out.shape}\"\n",
    "\n",
    "print(\"Conv2_out shape (should be half the height and width due to stride 2):\", conv2_out.shape)\n",
    "assert conv2_out.shape == (batch_size, 64, height//2, width//2), f\"conv2_out shape expected: {(batch_size, 64, height//2, width//2)}, obtained: {conv2_out.shape}\"\n",
    "\n",
    "print(\"Conv3_out shape (should be a quarter of the height and width due to stride 2):\", conv3_out.shape)\n",
    "assert conv3_out.shape == (batch_size, 128, height//4, width//4), f\"conv3_out shape expected: {(batch_size, 128, height//4, width//4)}, obtained: {conv3_out.shape}\"\n",
    "\n",
    "\n",
    "for name, tensor in [('conv1_out', conv1_out), ('conv2_out', conv2_out), ('conv3_out', conv3_out)]: \n",
    "    assert not torch.isnan(tensor).any(), f\"{name} contains NaN\" \n",
    "    assert not torch.isinf(tensor).any(), f\"{name} contains infinite values\"\n",
    "\n",
    "test_sizes = [(32, 32), (64, 64), (128, 128)]\n",
    "for h, w in test_sizes: \n",
    "    try: \n",
    "        X_test = torch.randn(batch_size, in_channels, h, w) \n",
    "        c1, c2, c3 = encoder(X_test) \n",
    "        assert c1.shape == (batch_size, 32, h, w) \n",
    "        assert c2.shape == (batch_size, 64, h//2, w//2) \n",
    "        assert c3.shape == (batch_size, 128, h//4, w//4)\n",
    "    except Exception as e:\n",
    "        assert False, f\"Error with input size {(h,w)}: {str(e)}\"\n",
    "\n",
    "print(\"\\nAll CNN Encoder tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8f26b",
   "metadata": {},
   "source": [
    "#### Test for CNN Decoder\n",
    "\n",
    "We test the 3 stage Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "base_height = 4\n",
    "base_width = 4\n",
    "encoder_in_channels = 3\n",
    "deep_channels = [32, 64, 128]\n",
    "\n",
    "# Simulated outputs from ConvLSTM layers\n",
    "conv1_lstm_out = torch.randn(batch_size, 32, base_height*8, base_width*8)\n",
    "conv2_lstm_out = torch.randn(batch_size, 64, base_height*4, base_width*4)\n",
    "conv3_lstm_out = torch.randn(batch_size, 128, base_height*2, base_width*2)\n",
    "\n",
    "decoder = CnnDecoder(in_channels_encoder=encoder_in_channels, deep_channel_sizes=deep_channels)\n",
    "output = decoder(conv1_lstm_out, conv2_lstm_out, conv3_lstm_out)\n",
    "\n",
    "expected_shape = (batch_size, encoder_in_channels, base_height*8, base_width*8)\n",
    "assert output.shape == expected_shape, f\"Expected shape: {expected_shape}, obtained: {output.shape}\"\n",
    "assert not torch.isnan(output).any(), \"Result contains NaNs\"\n",
    "assert not torch.isinf(output).any(), \"Result contains infinite values\"\n",
    "\n",
    "print(\"\\nTest with different input sizes:\")\n",
    "test_sizes = [(4, 4), (8, 8), (16, 16)]\n",
    "for h, w in test_sizes:\n",
    "    try:\n",
    "        conv1 = torch.randn(batch_size, 32, h*8, w*8)\n",
    "        conv2 = torch.randn(batch_size, 64, h*4, w*4)\n",
    "        conv3 = torch.randn(batch_size, 128, h*2, w*2)\n",
    "\n",
    "        out = decoder(conv1, conv2, conv3)\n",
    "        assert out.shape == (batch_size, 3, h*8, w*8), f\"Failed for base size {(h,w)}: got {out.shape}\"\n",
    "        print(f\"Passed for base size {(h,w)}: got {out.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        assert False, f\"Error with base size {(h,w)}: {str(e)}\"\n",
    "\n",
    "print(\"\\nAll CNN Decoder tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe4719",
   "metadata": {},
   "source": [
    "#### Test for ConvLSTM Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8aa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "height = 32\n",
    "width = 32\n",
    "layers = 2\n",
    "deep_channels = [32, 64, 128]\n",
    "timesteps = 5\n",
    "attention_timesteps = [1, 2, 3, 4]\n",
    "\n",
    "# Simulated outputs from Encoder\n",
    "conv1_out = torch.randn(batch_size, 32, height, width).to(device) # Conv1\n",
    "conv2_out = torch.randn(batch_size, 64, height//2, width//2).to(device) # Conv2\n",
    "conv3_out = torch.randn(batch_size, 128, height//4, width//4).to(device) # Conv3\n",
    "\n",
    "conv_lstm = Conv_LSTM(deep_channel_sizes=deep_channels, num_layers=layers, n_timesteps=timesteps, n_effective_timesteps=attention_timesteps).to(device)\n",
    "conv1_lstm, conv2_lstm, conv3_lstm = conv_lstm(conv1_out, conv2_out, conv3_out)\n",
    "print(\"Shape of conv1_lstm:\", conv1_lstm.shape)\n",
    "print(\"Shape of conv2_lstm:\", conv2_lstm.shape)\n",
    "print(\"Shape of conv3_lstm:\", conv3_lstm.shape)\n",
    "\n",
    "# Checking output shapes\n",
    "assert conv1_lstm.shape == (batch_size, 32, height, width), f\"conv1_lstm shape expected: {(batch_size, 32, height, width)}, obtained: {conv1_lstm.shape}\"\n",
    "assert conv2_lstm.shape == (batch_size, 64, height//2, width//2), f\"conv2_lstm shape expected: {(batch_size, 64, height//2, width//2)}, obtained: {conv2_lstm.shape}\"\n",
    "assert conv3_lstm.shape == (batch_size, 128, height//4, width//4), f\"conv3_lstm expected shape: {(batch_size, 128, height//4, width//4)}, got: {conv3_lstm.shape}\"\n",
    "\n",
    "# Checking values\n",
    "for name, tensor in [('conv1_lstm', conv1_lstm), ('conv2_lstm', conv2_lstm), ('conv3_lstm', conv3_lstm)]:\n",
    "    assert not torch.isnan(tensor).any(), f\"{name} contains NaNs\"\n",
    "    assert not torch.isinf(tensor).any(), f\"{name} contains infinite values\"\n",
    "    assert tensor.requires_grad, f\"{name} does not preserve the gradient\"\n",
    "\n",
    "# Test with different image sizes\n",
    "print(\"\\nTest with different input sizes:\")\n",
    "test_sizes = [(32, 32), (64, 64), (80, 80)]\n",
    "for h, w in test_sizes:\n",
    "    try:\n",
    "        # Recreate the inputs with the new dimensions\n",
    "        conv1 = torch.randn(batch_size, 32, h, w).to(device)\n",
    "        conv2 = torch.randn(batch_size, 64, h//2, w//2).to(device)\n",
    "        conv3 = torch.randn(batch_size, 128, h//4, w//4).to(device)\n",
    "        # Create and test the ConvLSTM \n",
    "        conv_lstm = Conv_LSTM(deep_channel_sizes=deep_channels, num_layers=layers, n_timesteps=timesteps, n_effective_timesteps=attention_timesteps) .to(device)\n",
    "        out1, out2, out3 = conv_lstm(conv1, conv2, conv3) \n",
    "        # Checking\n",
    "        assert out1.shape == (batch_size, 32, h, w)\n",
    "        assert out2.shape == (batch_size, 64, h//2, w//2) \n",
    "        assert out3.shape == (batch_size, 128, h//4, w//4)\n",
    "    except Exception as e: \n",
    "        assert False, f\"Error with shape {(h,w)}: {str(e)}\"\n",
    "    print(f\"Passed for shape {(h,w)}: out1 {out1.shape}, out2 {out2.shape}, out3 {out3.shape}\")\n",
    "\n",
    "print(\"\\nAll Conv_LSTM tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder-Decoder global model\n",
    "class MSCRED(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Scale Convolutional Recurrent Encoder-Decoder\n",
    "\n",
    "    This model combines CNN Autoencoder and ConvLSTM layers for spatiotemporal feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            encoder_in_channel: int, \n",
    "            deep_channel_sizes: list[int], \n",
    "            lstm_num_layers: int = 1,\n",
    "            lstm_timesteps: int = 5, \n",
    "            lstm_effective_timesteps: list[int] | str = 'all'\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        assert len(lstm_effective_timesteps) > 0 and len(lstm_effective_timesteps) <= lstm_timesteps, \"Effective timesteps must be non-empty and less than or equal to total timesteps\"\n",
    "\n",
    "        if lstm_effective_timesteps == 'all':\n",
    "            lstm_effective_timesteps = list(range(lstm_timesteps))\n",
    "\n",
    "        self.cnn_encoder = CnnEncoder(in_channels_encoder=encoder_in_channel, deep_channel_sizes=deep_channel_sizes)\n",
    "        self.conv_lstm = Conv_LSTM(deep_channel_sizes=deep_channel_sizes, num_layers=lstm_num_layers, n_timesteps=lstm_timesteps, n_effective_timesteps=lstm_effective_timesteps)\n",
    "        self.cnn_decoder = CnnDecoder(in_channels_encoder=encoder_in_channel, deep_channel_sizes=deep_channel_sizes)\n",
    "\n",
    "        self.time_steps = lstm_timesteps\n",
    "        self.model_depth = len(deep_channel_sizes)\n",
    "\n",
    "    def reset_lstm_hidden_states(self):\n",
    "        \"\"\"\n",
    "        Initializes all the hidden states (h,c) of the ConvLSTM layers.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'conv_lstm'):\n",
    "            for i in range(self.model_depth):\n",
    "                if hasattr(self.conv_lstm, f'conv{i+1}_lstm'):\n",
    "                    getattr(self.conv_lstm, f'conv{i+1}_lstm').reset_hidden_state()\n",
    "                else:\n",
    "                    raise AttributeError(f\"ConvLSTM layer conv{i+1}_lstm does not exist in Conv_LSTM module.\")\n",
    "        else:\n",
    "            raise AttributeError(\"ConvLSTM module does not exist.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the MSCRED model\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor. Shape (batch_size, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed tensor from the decoder\n",
    "        \"\"\"\n",
    "        assert x.dim() == 4, \"Input tensor must be 4D (batch_size, channels, height, width)\"\n",
    "        # assert x.shape[0] > self.time_steps, \"Input batch size must be greater than the number of time steps\"\n",
    "        # assert x.shape[0] % self.time_steps == 0, \"Input batch size must be divisible by the number of time steps\"\n",
    "\n",
    "        conv1_out, conv2_out, conv3_out = self.cnn_encoder(x)\n",
    "        conv1_lstm_out, conv2_lstm_out, conv3_lstm_out = self.conv_lstm(conv1_out, conv2_out, conv3_out)\n",
    "        gen_x = self.cnn_decoder(conv1_lstm_out, conv2_lstm_out, conv3_lstm_out)\n",
    "        \n",
    "        return gen_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be38e7c",
   "metadata": {},
   "source": [
    "#### Test MSCRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "in_channels = 3\n",
    "height = 32\n",
    "width = 32\n",
    "layers = 2\n",
    "deep_channels = [32, 64, 128]\n",
    "timesteps = 5\n",
    "effective_steps = [1, 2, 4]\n",
    "\n",
    "X = torch.randn(batch_size, in_channels, height, width).to(device)\n",
    "\n",
    "mscred = MSCRED(encoder_in_channel=in_channels, deep_channel_sizes=deep_channels, lstm_num_layers=layers, lstm_timesteps=timesteps, lstm_effective_timesteps=effective_steps).to(device)\n",
    "X_recon = mscred(X).cpu()\n",
    "print(\"Shape of X_recon:\", X_recon.shape)\n",
    "\n",
    "# Checking output shapes\n",
    "assert X.shape == X_recon.shape, f\"X shape expected: {X.shape}, obtained: {X_recon.shape}\"\n",
    "\n",
    "# Checking values\n",
    "\n",
    "assert not torch.isnan(X_recon).any(), f\"X_recon contains NaNs\"\n",
    "assert not torch.isinf(X_recon).any(), f\"X_recon contains infinite values\"\n",
    "assert X_recon.requires_grad, f\"X_recon does not preserve the gradient\"\n",
    "\n",
    "# Test with different image sizes\n",
    "print(\"\\nTest with different input sizes:\")\n",
    "test_sizes = [(32, 32), (36, 36), (28, 28)]\n",
    "for h, w in test_sizes:\n",
    "    try:\n",
    "        # Recreate the inputs with the new dimensions\n",
    "        X_test = torch.randn(batch_size, in_channels, h, w).to(device)\n",
    "        # Create and test the ConvLSTM \n",
    "        mscred = MSCRED(encoder_in_channel=in_channels, deep_channel_sizes=deep_channels, lstm_num_layers=layers, lstm_timesteps=timesteps, lstm_effective_timesteps=effective_steps).to(device)\n",
    "        X_recon = mscred(X_test).cpu()\n",
    "        # Checking\n",
    "        assert X_recon.shape == (batch_size, in_channels, h, w)\n",
    "    except Exception as e: \n",
    "        assert False, f\"Error with shape {(h,w)}: {str(e)}\"\n",
    "    print(f\"Passed for shape {(h,w)}: {X_recon.shape}\")\n",
    "\n",
    "print(\"\\nAll MSCRED tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf53e4",
   "metadata": {},
   "source": [
    "## 3.bis. Test the gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient_flow(model, sample_batch):\n",
    "    model.train()\n",
    "    output = model(sample_batch)\n",
    "    loss = torch.mean(output)       # Simple loss for testing\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check if encoder parameters receive gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'cnn_encoder' in name and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            if grad_norm > 1e-6:\n",
    "                print(f\"Encoder gradient flow: {name} - {grad_norm}\")\n",
    "    \n",
    "    # Check if ConvLSTM parameters receive gradients  \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv_lstm' in name and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            if grad_norm > 1e-6:\n",
    "                print(f\"ConvLSTM gradient flow: {name} - {grad_norm}\")\n",
    "\n",
    "    # Check if decoder parameters receive gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'cnn_decoder' in name and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            if grad_norm > 1e-6:\n",
    "                print(f\"Decoder gradient flow: {name} - {grad_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781c7f6",
   "metadata": {},
   "source": [
    "## 4. DATA loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e91b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, device: str = \"cpu\", start_idx: int = 0, end_idx: int = -1):\n",
    "        self.data = torch.from_numpy(data[start_idx:end_idx]).float().to(device)       # Convert to float32 to be compatible with PyTorch model parameters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "gap_time = 10\n",
    "total_timesteps = 20000//gap_time\n",
    "train_end = 12000//gap_time\n",
    "valid_start = 12000//gap_time\n",
    "valid_end = 15000//gap_time\n",
    "test_start = 15000//gap_time\n",
    "\n",
    "# Load data (*.cn or synthetic one)\n",
    "# data = np.load('votre_fichier.nc')  # shape [2000, 3, 32, 32]\n",
    "#data = torch.randn(2000, 3, 32, 32)  # Exemple avec des données aléatoires\n",
    "data = signature_matrices\n",
    "\n",
    "# Datasets\n",
    "train_dataset = TimeSeriesDataset(data, device, 0, train_end)\n",
    "valid_dataset = TimeSeriesDataset(data, device, valid_start, valid_end)\n",
    "test_dataset = TimeSeriesDataset(data, device, test_start, total_timesteps)\n",
    "assert len(train_dataset) % batch_size == 0, f\"Batch size {batch_size} must divide {len(train_dataset)}\"\n",
    "assert len(valid_dataset) % batch_size == 0, f\"Batch size {batch_size} must divide {len(valid_dataset)}\"\n",
    "assert len(test_dataset) % batch_size == 0, f\"Batch size {batch_size} must divide {len(test_dataset)}\"\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,      # Very important\n",
    "    drop_last=False     # Either that with the assertion or drop_last=True without assertion (for more freedom)\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Batch size:    \", batch_size)\n",
    "print(f\"Train set:      {len(train_loader)}\")\n",
    "print(f\"Validation set: {len(valid_loader)}\")\n",
    "print(f\"Test set:       {len(test_loader)}\")\n",
    "for batch in train_loader:\n",
    "    print(f\"Set shape : {batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec618f",
   "metadata": {},
   "source": [
    "## 5. Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed: torch.Tensor, original: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Loss function for measuring reconstruction error.\n",
    "\n",
    "    Nota bene:\n",
    "        - We sum over the spatial dimensions (window_size, height and width).\n",
    "        - We also take the mean over the batch dimension (corresponding to time here).\n",
    "    \"\"\"\n",
    "    squared_error = (reconstructed - original) ** 2\n",
    "    loss = torch.mean(torch.sum(squared_error, dim=(1, 2, 3)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac75620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        \n",
    "        # Register initial shadow values\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    def on_batch_end(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data\n",
    "    \n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec48c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping:\n",
    "    def __init__(self, max_norm=1.0):\n",
    "        self.max_norm = max_norm\n",
    "    \n",
    "    def on_backward_end(self, model):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), self.max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to terminate training when a monitored metric has stopped improving.\n",
    "\n",
    "    Methods:\n",
    "    check_stop(current_loss: float, model) -> bool:\n",
    "        Check if training should be stopped based on the current loss.\n",
    "    \n",
    "    restore_best_weights(model) -> None:\n",
    "        Restore the model weights from the best epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_delta: float = 0.001, patience: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        min_delta: Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        patience: Number of epochs with no improvement after which training will be stopped.\n",
    "        \"\"\"\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_state_dict = None\n",
    "\n",
    "    def check_stop(self, current_loss: float, model: nn.Module) -> bool:\n",
    "        \"\"\"\n",
    "        Check if training should be stopped based on the current loss.\n",
    "\n",
    "        Parameters:\n",
    "        current_loss: The loss value for the current epoch.\n",
    "        model: The model whose state_dict will be saved if it is the best so far.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            self.best_state_dict = model.state_dict()  # <- Save initial weights\n",
    "            return False\n",
    "\n",
    "        relative_change = abs((self.best_loss - current_loss) / (self.best_loss + 1e-8))\n",
    "\n",
    "        if relative_change < self.min_delta:\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            self.best_state_dict = model.state_dict()  # <- Save new best weights\n",
    "\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore_best_weights(self, model: nn.Module) -> None:\n",
    "        \"\"\"\n",
    "        Restore the model weights from the best epoch.\n",
    "\n",
    "        Parameters:\n",
    "        model: The model whose weights will be restored.\n",
    "        \"\"\"\n",
    "        if self.best_state_dict is not None:\n",
    "            model.load_state_dict(self.best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceLROnPlateau:\n",
    "    \"\"\"\n",
    "    Reduce learning rate when a metric has stopped improving.\n",
    "    \n",
    "    Methods:\n",
    "    step(current_loss: float) -> bool:\n",
    "        Update the learning rate based on the current loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            optimizer: torch.optim.Optimizer, \n",
    "            mode: str = 'min', \n",
    "            factor: float = 0.1, \n",
    "            patience: int = 10, \n",
    "            min_lr: float = 1e-6, \n",
    "            min_delta: float = 1e-4\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Reduce learning rate when a metric has stopped improving.\n",
    "        \n",
    "        Parameters:\n",
    "        optimizer: The optimizer whose learning rate will be reduced.\n",
    "        mode: One of 'min' or 'max'. In 'min' mode, lr will be reduced when the quantity stopped decreasing. In 'max', when it stops increasing.\n",
    "        factor: Factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "        patience: Number of epochs with no improvement after which learning rate will be reduced.\n",
    "        min_lr: Lower bound on the learning rate.\n",
    "        min_delta: Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.mode = mode\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.last_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "\n",
    "    def step(self, current_loss: float) -> bool:\n",
    "        \"\"\"\n",
    "        Update the learning rate based on the current loss.\n",
    "        \n",
    "        Parameters:\n",
    "        current_loss: The current value of the monitored metric.\n",
    "        \n",
    "        Returns:\n",
    "        bool: True if the learning rate was reduced, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.best is None:\n",
    "            self.best = current_loss\n",
    "            return False\n",
    "            \n",
    "        if self.mode == 'min':\n",
    "            is_better = current_loss < (self.best - self.min_delta)\n",
    "        else:  # mode == 'max'\n",
    "            is_better = current_loss > (self.best + self.min_delta)\n",
    "        \n",
    "        if is_better:\n",
    "            self.best = current_loss\n",
    "            self.num_bad_epochs = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "            \n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self._reduce_lr()\n",
    "            self.num_bad_epochs = 0\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def _reduce_lr(self) -> None:\n",
    "        \"\"\"Reduce learning rate for all parameter groups.\"\"\"\n",
    "        for _, param_group in enumerate(self.optimizer.param_groups):\n",
    "            old_lr = param_group['lr']\n",
    "            new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "            \n",
    "            if new_lr != old_lr:\n",
    "                param_group['lr'] = new_lr\n",
    "                print(f\"Reduced learning rate from {old_lr:.2e} to {new_lr:.2e}\")\n",
    "                \n",
    "        self.last_lr = [group['lr'] for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, dataLoader: DataLoader, optimizer: torch.optim.Optimizer, epochs: int, device: torch.device, valid_loader: DataLoader = None):\n",
    "    model = model.to(device)\n",
    "    print(\"------training on {}-------\".format(device))\n",
    "\n",
    "    early_stopping = EarlyStopping(min_delta=0.01, patience=6)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, min_lr=1e-6, min_delta=0.001)\n",
    "\n",
    "    history = {'train_loss': [], 'valid_loss': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.reset_lstm_hidden_states()\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_sum, n = 0.0, 0\n",
    "        for x in tqdm(dataLoader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_recon = model(x)\n",
    "            loss = loss_function(x_recon, x)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.item()\n",
    "            n += 1\n",
    "            \n",
    "        train_loss = train_loss_sum / n\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "\n",
    "        # Validation (if valid_loader is not None)\n",
    "        valid_loss = None\n",
    "        if valid_loader is not None:\n",
    "            model.eval()\n",
    "            valid_loss_sum, valid_n = 0.0, 0\n",
    "            with torch.no_grad():\n",
    "                for x_valid in valid_loader:\n",
    "                    x_valid = x_valid.to(device)\n",
    "                    x_recon_valid = model(x_valid)\n",
    "                    loss_valid = loss_function(x_recon_valid, x_valid)\n",
    "                    valid_loss_sum += loss_valid.item()\n",
    "                    valid_n += 1\n",
    "            valid_loss = valid_loss_sum / valid_n\n",
    "            history['valid_loss'].append(valid_loss)\n",
    "\n",
    "        current_loss = valid_loss if valid_loader is not None else train_loss\n",
    "\n",
    "        print(\"[Epoch %d/%d] [Train loss: %f] %s\" % (\n",
    "            epoch+1, epochs, train_loss, \n",
    "            f\"[Val loss: {valid_loss:.4f}]\" if valid_loss is not None else \"No valid\"\n",
    "        ))\n",
    "        \n",
    "        if early_stopping.check_stop(current_loss, model):\n",
    "            print(f\"Early stopping at epoch {epoch + 1} with loss {current_loss:.4f}\")\n",
    "            early_stopping.restore_best_weights(model)\n",
    "            break\n",
    "\n",
    "        lr_scheduler.step(current_loss)\n",
    "\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mscred = MSCRED(\n",
    "    encoder_in_channel=3,\n",
    "    deep_channel_sizes=[32, 64, 128],\n",
    "    lstm_num_layers=2,\n",
    "    lstm_timesteps=5,\n",
    "    lstm_effective_timesteps=[1, 3, 4]\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d74520",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_loader:\n",
    "    test_gradient_flow(mscred, x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58235a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "n_epochs = 12\n",
    "path = Path().absolute().parent.parent / \"results/model_params/mscred\"\n",
    "model_name_continue = \"model0\"\n",
    "model_name_register = \"model1\"\n",
    "continue_training = False\n",
    "if continue_training:\n",
    "    mscred.load_state_dict(torch.load(path / f\"{model_name_continue}.pth\"))\n",
    "optimizer = torch.optim.Adam(mscred.parameters(), lr = 1.0e-4)\n",
    "history, trained_mscred = train(mscred, train_loader, optimizer, n_epochs, device, valid_loader)\n",
    "torch.save(trained_mscred.state_dict(), path / f\"{model_name_register}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history_train, history_valid):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history_train, 'b-', linewidth=2, label='Train Loss')\n",
    "    plt.plot(history_valid, 'r-', linewidth=2, label='Valid Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history['train_loss'], history['valid_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1048c5",
   "metadata": {},
   "source": [
    "## 5. Test the model and plot reconstruction & residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_score(reconstruction, original):\n",
    "    # Shape: (batch_size, channel, height, width)\n",
    "    residuals = np.abs(reconstruction - original)\n",
    "    anomaly_score = np.sum(residuals, axis=(1, 2, 3))\n",
    "    return anomaly_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_testing_anomaly_threshold(validation_anomaly_scores, alpha=1.5):\n",
    "    \"\"\"\n",
    "    Trouve le seuil d'anomalie pour les données de test en utilisant l'algorithme de Tukey.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_scores: Scores d'anomalie\n",
    "        alpha: Coefficient multiplicateur pour le seuil d'anomalie\n",
    "    \n",
    "    Returns:\n",
    "        threshold: Seuil d'anomalie\n",
    "    \"\"\"\n",
    "    s_valid = np.max(validation_anomaly_scores)\n",
    "    threshold = s_valid * alpha\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5978e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50295ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e99352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3609d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_detect_anomalies(model, test_loader, gap_time=10, test_start_point=10000, test_end_point=20000,\n",
    "                              valid_start_point=8000, valid_end_point=10000, thred_broken=0.005, alpha=1.5):\n",
    "    \"\"\"\n",
    "    Teste le modèle et détecte les anomalies\n",
    "    \n",
    "    Args:\n",
    "        model: Modèle MSCRED entraîné\n",
    "        test_loader: DataLoader pour les données de test\n",
    "        gap_time: Intervalle de temps entre les matrices de signature\n",
    "        test_start_point: Point de début du test en steps originaux\n",
    "        test_end_point: Point de fin du test en steps originaux\n",
    "        valid_start_point: Point de début de la validation en steps originaux\n",
    "        valid_end_point: Point de fin de la validation en steps originaux\n",
    "        thred_broken: Seuil pour les pixels cassés\n",
    "        alpha: Coefficient multiplicateur pour le seuil d'anomalie\n",
    "    \n",
    "    Returns:\n",
    "        reconstruction: Matrices reconstruites\n",
    "        residuals: Matrices résiduelles\n",
    "        anomaly_score: Scores d'anomalie\n",
    "    \"\"\"\n",
    "    print(\"------Testing-------\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialisation des arrays pour stocker les résultats\n",
    "    reconstruction = []\n",
    "    residuals = []\n",
    "    anomaly_score = []\n",
    "    \n",
    "    # Conversion des points en indices de matrice\n",
    "    valid_start = valid_start_point // gap_time\n",
    "    valid_end = valid_end_point // gap_time\n",
    "    test_start = test_start_point // gap_time\n",
    "    test_end = test_end_point // gap_time\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, x in enumerate(tqdm(test_loader)):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Reconstruction\n",
    "            reconstructed_matrix = model(x)\n",
    "            \n",
    "            # Calcul des résidus\n",
    "            residual = torch.abs(reconstructed_matrix - x)\n",
    "            residuals.append(residual.cpu())\n",
    "            \n",
    "            # Stockage de la reconstruction\n",
    "            reconstruction.append(reconstructed_matrix.cpu())\n",
    "            \n",
    "            # Calcul du score d'anomalie (nombre de pixels cassés)\n",
    "            for j in range(x.size(0)):\n",
    "                residual_matrix = residual[j]\n",
    "                num_broken = torch.sum(residual_matrix > thred_broken).item()\n",
    "                anomaly_score.append(num_broken)\n",
    "    \n",
    "    # Conversion en tensors\n",
    "    reconstruction = torch.cat(reconstruction, dim=0)\n",
    "    residuals = torch.cat(residuals, dim=0)\n",
    "    anomaly_score = np.array(anomaly_score)\n",
    "    \n",
    "    return reconstruction, residuals, anomaly_score\n",
    "\n",
    "# Fonction pour visualiser les scores d'anomalie\n",
    "def plot_anomaly_scores(anomaly_score, test_start, test_end, valid_start, valid_end, \n",
    "                        gap_time, alpha, anomalies_info=None):\n",
    "    \"\"\"\n",
    "    Trace les scores d'anomalie avec les zones d'anomalies ground truth\n",
    "    \"\"\"\n",
    "    # Calcul du seuil basé sur la période de validation\n",
    "    valid_scores = anomaly_score[valid_start:valid_end]\n",
    "    valid_anomaly_max = np.max(valid_scores)\n",
    "    threshold = valid_anomaly_max * alpha\n",
    "    \n",
    "    # Préparation du graphique\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Tracé des scores d'anomalie\n",
    "    test_num = test_end - test_start\n",
    "    x_values = np.arange(test_num)\n",
    "    ax.plot(x_values, anomaly_score[test_start:test_end], color='black', linewidth=2, label='Anomaly Score')\n",
    "    \n",
    "    # Ligne de seuil\n",
    "    ax.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold:.2f})')\n",
    "    \n",
    "    # Ajout des zones d'anomalies ground truth si disponibles\n",
    "    if anomalies_info:\n",
    "        for info in anomalies_info:\n",
    "            start_idx = (info['start_index'] - test_start * gap_time) // gap_time\n",
    "            duration = info['duration'] // gap_time\n",
    "            ax.axvspan(start_idx, start_idx + duration, color='red', alpha=0.3, label='Anomaly Ground Truth' if 'Anomaly Ground Truth' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "    \n",
    "    ax.set_xlabel('Test Time (matrix index)')\n",
    "    ax.set_ylabel('Anomaly Score')\n",
    "    ax.set_title('Anomaly Detection Results')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/anomaly_score.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return threshold\n",
    "\n",
    "# Fonction pour visualiser les matrices de signature\n",
    "def plot_signature_matrices(original, reconstructed, residual, indices, titles):\n",
    "    \"\"\"\n",
    "    Affiche les matrices originales, reconstruites et résiduelles pour des indices donnés\n",
    "    \"\"\"\n",
    "    n_indices = len(indices)\n",
    "    fig, axes = plt.subplots(n_indices, 3, figsize=(15, 5*n_indices))\n",
    "    \n",
    "    if n_indices == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Matrice originale\n",
    "        axes[i, 0].imshow(original[idx].mean(dim=0), cmap='viridis')\n",
    "        axes[i, 0].set_title(f'{titles[i]} - Original (Index {idx})')\n",
    "        axes[i, 0].set_xlabel('Width')\n",
    "        axes[i, 0].set_ylabel('Height')\n",
    "        \n",
    "        # Matrice reconstruite\n",
    "        axes[i, 1].imshow(reconstructed[idx].mean(dim=0), cmap='viridis')\n",
    "        axes[i, 1].set_title(f'{titles[i]} - Reconstructed (Index {idx})')\n",
    "        axes[i, 1].set_xlabel('Width')\n",
    "        axes[i, 1].set_ylabel('Height')\n",
    "        \n",
    "        # Matrice résiduelle\n",
    "        res_mean = residual[idx].mean(dim=0)\n",
    "        im = axes[i, 2].imshow(res_mean, cmap='hot')\n",
    "        axes[i, 2].set_title(f'{titles[i]} - Residual (Index {idx})\\nMean: {res_mean.mean():.4f}, Max: {res_mean.max():.4f}')\n",
    "        axes[i, 2].set_xlabel('Width')\n",
    "        axes[i, 2].set_ylabel('Height')\n",
    "        plt.colorbar(im, ax=axes[i, 2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./outputs/signature_matrices_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour analyser les performances de détection\n",
    "def analyze_detection_performance(anomaly_score, threshold, anomalies_info, test_start, gap_time):\n",
    "    \"\"\"\n",
    "    Analyse les performances de détection d'anomalies\n",
    "    \"\"\"\n",
    "    # Détection des anomalies basée sur le seuil\n",
    "    detected_anomalies = anomaly_score > threshold\n",
    "    \n",
    "    # Calcul des métriques si les ground truth sont disponibles\n",
    "    if anomalies_info:\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        \n",
    "        # Création d'un array pour les anomalies ground truth\n",
    "        gt_anomalies = np.zeros_like(anomaly_score, dtype=bool)\n",
    "        for info in anomalies_info:\n",
    "            start_idx = (info['start_index'] - test_start * gap_time) // gap_time\n",
    "            end_idx = start_idx + (info['duration'] // gap_time)\n",
    "            gt_anomalies[start_idx:end_idx] = True\n",
    "        \n",
    "        # Calcul des vrais positifs, faux positifs et faux négatifs\n",
    "        tp = np.sum(detected_anomalies & gt_anomalies)\n",
    "        fp = np.sum(detected_anomalies & ~gt_anomalies)\n",
    "        fn = np.sum(~detected_anomalies & gt_anomalies)\n",
    "        \n",
    "        # Calcul de la précision et du rappel\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"True Positives: {tp}\")\n",
    "        print(f\"False Positives: {fp}\")\n",
    "        print(f\"False Negatives: {fn}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return detected_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b268fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0f065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataLoader):\n",
    "    print(\"------Testing-------\")\n",
    "    reconstructed_data_path = \"../data/matrix_data/reconstructed_data/\"\n",
    "    os.makedirs(reconstructed_data_path, exist_ok=True)\n",
    "    for element in dataLoader:\n",
    "        element_shape = element.shape\n",
    "        break\n",
    "    reconstruction = torch.zeros((len(dataLoader)*element_shape[0], *element_shape[1:]))\n",
    "    loss = 0\n",
    "    i = 0\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(dataLoader):\n",
    "            x = x.to(device)\n",
    "            x = x.squeeze()\n",
    "            reconstructed_matrix = model(x) \n",
    "            \n",
    "            l = loss_function(reconstructed_matrix, x)\n",
    "            loss += l.item()\n",
    "            #print(\"[test_index %d] [loss: %f]\" % (i, l.item()))\n",
    "\n",
    "            reconstruction[i*element_shape[0]:(i+1)*element_shape[0]] = reconstructed_matrix.cpu().detach()\n",
    "            i += 1\n",
    "        loss /= len(dataLoader)\n",
    "        print(\"Final loss: [loss: %f]\" % (loss))\n",
    "\n",
    "        path_temp = os.path.join(reconstructed_data_path, \"reconstructed_data.npy\")\n",
    "        np.save(path_temp, reconstruction.cpu().detach().numpy())\n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "mscred.load_state_dict(torch.load(\"./checkpoints/model1.pth\"))\n",
    "mscred.to(device)\n",
    "reconstruction = test(mscred, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da00141",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signature_matrices(\n",
    "    matrix_4d=reconstruction,\n",
    "    sample_times=[0, 200, 300, 500]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed228d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the residual matrix between reconstruction matrices and signature matrices\n",
    "residuals = torch.abs(reconstruction - signature_matrices[15000//gap_time:20000//gap_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the score of anomaly (scalar)\n",
    "anomaly_score = torch.sum(residuals, dim=(1, 2, 3)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by ascending anomaly score\n",
    "sorted_indices = np.argsort(anomaly_score)\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 5 most anomalous samples\n",
    "# Plot the 3 residual channels for the 5 worst reconstructions\n",
    "for i in range(5):\n",
    "    index = sorted_indices[i]\n",
    "    \n",
    "    # Create figure with 1 row and 3 columns for the 3 residual channels\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Get residual matrix for this sample (shape: [3, 32, 32])\n",
    "    residual_matrix = residuals[index].cpu().detach().numpy()\n",
    "    \n",
    "    # Plot each residual channel separately\n",
    "    for channel in range(3):\n",
    "        im = axes[channel].imshow(residual_matrix[channel], \n",
    "                                 aspect='auto', \n",
    "                                 cmap='hot',  # Colormap appropriée pour les résidus\n",
    "                                 vmin=0,      # Les résidus sont toujours positifs\n",
    "                                 vmax=residual_matrix.max())  # Même échelle pour tous les canaux\n",
    "        \n",
    "        axes[channel].set_title(f'Channel {channel} Residual\\nMax: {residual_matrix[channel].max():.3f}')\n",
    "        axes[channel].set_xlabel('Width')\n",
    "        axes[channel].set_ylabel('Height')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[channel], shrink=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed91861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe76fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b223e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5edbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f65da39",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITER-magnetics-diagnostic-analysis (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
